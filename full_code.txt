Directory Tree:
screencapturekit-node/
│   ├── tsup.config.js
│   ├── license
│   ├── README.md
│   ├── Package.resolved
│   ├── .gitignore
│   ├── tsup.config.ts
│   ├── Package.swift
│   ├── tsconfig.json
│   ├── .build/ [EXCLUDED]
│   ├── dist/ [EXCLUDED]
│   ├── example/
│   │   ├── audio-capture.js
│   │   ├── screen-with-audio.js
│   │   ├── index.js
│   │   ├── README.md
│   │   ├── screen-only.js
│   │   ├── list-devices.js
│   │   ├── audio-only.js
│   ├── node_modules/ [EXCLUDED]
│   ├── Sources/
│   │   ├── screencapturekit-cli/
│   │   │   ├── Utilities.swift
│   │   │   ├── ScreenCaptureKitCli.swift
│   ├── .git/ [EXCLUDED]
│   ├── .vscode/
│   │   ├── launch.json
│   ├── src/
│   │   ├── index.test.js
│   │   ├── index.ts
│   │   ├── types/
│   │   │   ├── file-url.d.ts




# ======================
# File: tsup.config.js
# ======================

import { defineConfig } from "tsup";

const isProduction = process.env.NODE_ENV === "production";

export default defineConfig(({ watch = false }) => ({
  clean: true,
  dts: true,
  entry: {
    index: "src/index.ts",
  },
  external: [],
  format: ["esm"],
  minify: isProduction,
  sourcemap: isProduction,
  watch,
}));


# ======================
# File: README.md
# ======================

# ScreenCaptureKit Node.js

A Node.js wrapper for Apple's `ScreenCaptureKit` module. This package allows screen recording on macOS with optimal performance using Apple's native APIs.

## Features

- High-performance screen recording
- HDR (High Dynamic Range) support for macOS 13+ (Ventura)
- System audio capture
- Microphone audio capture (macOS 15+)
- Direct-to-file recording (simplified API for macOS 15+)
- Post-processing capabilities for audio tracks with FFmpeg
- Cropping support (capture specific screen areas)
- Multiple options control (FPS, cursor display, click highlighting)
- Support for various video codecs (H264, HEVC, ProRes)
- Listing available screens and audio devices

## Requirements

- macOS 10.13 (High Sierra) or newer
- Node.js 14 or newer
- FFmpeg (for post-processing audio tracks)

### FFmpeg Installation

FFmpeg is required for post-processing audio tracks. Here's how to install it on different systems:

#### macOS
Using Homebrew:
```bash
brew install ffmpeg
```

#### Linux (Debian/Ubuntu)
Using apt package manager:
```bash
sudo apt update && sudo apt install ffmpeg
```

#### Windows
You have several options:

1. **Using Chocolatey** (recommended if you have Chocolatey installed):
```bash
choco install ffmpeg
```

2. **Using the MSI Installer** (easiest method):
   - Download the [FFmpeg Installer](https://github.com/icedterminal/ffmpeg-installer/releases) from GitHub
   - Run the MSI file and follow the installation wizard
   - FFmpeg will be automatically added to your system PATH

3. **Manual Installation**:
   - Download FFmpeg from [ffmpeg.org](https://ffmpeg.org/download.html)
   - Extract the archive
   - Add FFmpeg to your system PATH manually

To verify the installation on any system, open a terminal/command prompt and run:
```bash
ffmpeg -version
```

## Installation

```bash
npm install screencapturekit
```

## Usage

### Simple Screen Recording

```javascript
import createScreenRecorder from 'screencapturekit';

const recorder = createScreenRecorder();

// Start recording
await recorder.startRecording();

// Wait for desired duration...
setTimeout(async () => {
  // Stop recording
  const videoPath = await recorder.stopRecording();
  console.log('Video recorded at:', videoPath);
}, 5000);
```

### Recording with Advanced Options

```javascript
import createScreenRecorder from 'screencapturekit';

const recorder = createScreenRecorder();

// Start recording with options
await recorder.startRecording({
  fps: 60,
  showCursor: true,
  highlightClicks: true,
  screenId: 0,
  videoCodec: 'h264',
  enableHDR: true, // Enable HDR recording (macOS 13+)
  microphoneDeviceId: 'device-id', // Enable microphone capture (macOS 15+)
  recordToFile: true, // Use direct recording API (macOS 15+)
  cropArea: {
    x: 0,
    y: 0,
    width: 1920,
    height: 1080
  }
});

// Wait...

// Stop recording
const videoPath = await recorder.stopRecording();
```

### List Available Screens

```javascript
import { screens } from 'screencapturekit';

const availableScreens = await screens();
console.log(availableScreens);
```

### List Audio Devices

```javascript
import { audioDevices, microphoneDevices } from 'screencapturekit';

// System audio devices
const systemAudio = await audioDevices();
console.log(systemAudio);

// Microphone devices
const mics = await microphoneDevices();
console.log(mics);
```

### Check Support for Features

```javascript
import { supportsHDRCapture, supportsDirectRecordingAPI, supportsMicrophoneCapture } from 'screencapturekit';

if (supportsHDRCapture) {
  console.log('Your system supports HDR capture');
}

if (supportsDirectRecordingAPI) {
  console.log('Your system supports direct-to-file recording');
}

if (supportsMicrophoneCapture) {
  console.log('Your system supports microphone capture');
}
```

## Recording Options

| Option | Type | Default | Description |
|--------|------|------------|-------------|
| fps | number | 30 | Frames per second |
| cropArea | object | undefined | Cropping area {x, y, width, height} |
| showCursor | boolean | true | Display cursor in recording |
| highlightClicks | boolean | false | Highlight mouse clicks |
| screenId | number | 0 | ID of screen to capture |
| audioDeviceId | number | undefined | System audio device ID |
| microphoneDeviceId | string | undefined | Microphone device ID (macOS 15+) |
| videoCodec | string | 'h264' | Video codec ('h264', 'hevc', 'proRes422', 'proRes4444') |
| enableHDR | boolean | false | Enable HDR recording (macOS 13+) |
| recordToFile | boolean | false | Use direct recording API (macOS 15+) |
| audioOnly | boolean | false | Record audio only, will convert to mp3 after recording |

## Post-processing

When both system audio and microphone are recorded, the library uses FFmpeg to merge these tracks into a single video file. This happens automatically in the `stopRecording()` method. Make sure you have FFmpeg installed on your system.

## Development

```bash
npm install
npm run build
```

## Tests

```bash
npm test
```

## License

MIT


# ======================
# File: tsup.config.ts
# ======================

import { defineConfig } from 'tsup'

export default defineConfig({
  entry: ['src/index.ts'],
  format: ['cjs', 'esm'],
  dts: true,
  clean: true,
  splitting: false,
  sourcemap: true,
  minify: false,
}) 

# ======================
# File: Package.swift
# ======================

// swift-tools-version: 5.9
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "screencapturekit-cli",
    platforms: [.macOS(.v13)], // HDR nécessite macOS 13+, certaines fonctionnalités microphone nécessitent macOS 15+
    dependencies: [
        .package(url: "https://github.com/apple/swift-argument-parser.git", from: "1.2.2"),
    ],
    targets: [
        // Targets are the basic building blocks of a package, defining a module or a test suite.
        // Targets can depend on other targets in this package and products from dependencies.
        .executableTarget(
            name: "screencapturekit",
            dependencies: [
                .product(name: "ArgumentParser", package: "swift-argument-parser"),
            ],
            path: "Sources"
        ),
    ]
)


# ======================
# File: tsconfig.json
# ======================

{
  "$schema": "https://json.schemastore.org/tsconfig",
  "display": "Node 18",

  "compilerOptions": {
    "lib": ["es2022"],
    "module": "NodeNext",
    "target": "es2022",

    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "typeRoots": ["./node_modules/@types", "./src/types"],

    "outDir": "dist"
  },
  "ts-node": {
    "esm": true
  },
  "include": ["src/**/*"]
}


# ======================
# File: example/audio-capture.js
# ======================

// Exemple de capture audio (système et microphone)
import createScreenRecorder from 'screencapturekit';
import { screens, audioDevices, microphoneDevices } from 'screencapturekit';
import { exec } from 'child_process';
import { promisify } from 'util';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import readline from 'readline';
import fs from 'fs';
import path from 'path';
import os from 'os';

const execAsync = promisify(exec);
const __dirname = dirname(fileURLToPath(import.meta.url));

// Durée d'enregistrement en millisecondes
const RECORDING_DURATION = 15000; // 15 secondes

// Créer une interface readline pour l'interaction utilisateur
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

// Fonction pour poser une question et obtenir une réponse
function question(query) {
  return new Promise(resolve => rl.question(query, resolve));
}

// Fonction pour choisir un périphérique dans une liste
async function chooseDevice(devices, type) {
  if (!devices || devices.length === 0) {
    return null;
  }
  
  console.log(`\nPériphériques ${type} disponibles:`);
  devices.forEach((device, index) => {
    console.log(`   [${index}] ${device.name} (${device.manufacturer || 'Fabricant inconnu'}) (ID=${device.id})`);
  });
  
  const defaultChoice = 0;
  const input = await question(`Choisissez un périphérique ${type} [0-${devices.length - 1}] (défaut: ${defaultChoice}): `);
  const choice = input === '' ? defaultChoice : parseInt(input, 10);
  
  if (isNaN(choice) || choice < 0 || choice >= devices.length) {
    console.log(`Choix invalide, utilisation du périphérique ${defaultChoice}`);
    return devices[defaultChoice];
  }
  
  return devices[choice];
}

async function openYouTubeInBrowser(url) {
  console.log(`Ouverture de ${url} dans le navigateur par défaut...`);
  try {
    await execAsync(`open "${url}"`);
    return true;
  } catch (error) {
    console.error(`Erreur lors de l'ouverture du navigateur: ${error.message}`);
    return false;
  }
}

// Fonction pour générer un nom de fichier audio unique
function generateAudioFileName() {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
  return path.join(os.tmpdir(), `audio-capture-${timestamp}.m4a`);
}

async function main() {
  try {
    console.log('=== CONFIGURATION DE CAPTURE AUDIO ===');
    
    // Obtenir les écrans disponibles (nécessaire même pour l'audio uniquement)
    const availableScreens = await screens();
    if (!availableScreens || availableScreens.length === 0) {
      throw new Error('Aucun écran disponible pour l\'enregistrement, nécessaire même pour l\'audio');
    }
    
    // Utiliser le premier écran disponible
    const selectedScreen = availableScreens[0];
    console.log(`\nÉcran utilisé pour la capture (nécessaire pour l'API): ${selectedScreen.width}x${selectedScreen.height}`);
    
    // Obtenir les périphériques audio système
    const systemAudioDevices = await audioDevices();
    let selectedAudioDevice = null;
    let captureSystemAudio = false;
    
    if (!systemAudioDevices || systemAudioDevices.length === 0) {
      console.warn('⚠️ Aucun périphérique audio système disponible');
    } else {
      // Demander si l'utilisateur veut capturer l'audio système
      const captureAudio = await question('\nVoulez-vous capturer l\'audio système? (O/n): ');
      captureSystemAudio = captureAudio.toLowerCase() !== 'n';
      
      if (captureSystemAudio) {
        // Choisir un périphérique audio
        selectedAudioDevice = await chooseDevice(systemAudioDevices, 'audio');
        if (selectedAudioDevice) {
          console.log(`\n✅ Périphérique audio sélectionné: ${selectedAudioDevice.name} (ID=${selectedAudioDevice.id})`);
        }
      }
    }
    
    // Obtenir les microphones
    let micDevices = [];
    let selectedMic = null;
    let captureMicrophone = false;
    
    try {
      micDevices = await microphoneDevices();
      
      if (!micDevices || micDevices.length === 0) {
        console.warn('⚠️ Aucun microphone disponible');
      } else {
        // Demander si l'utilisateur veut capturer le microphone
        const captureMic = await question('\nVoulez-vous capturer le microphone? (O/n): ');
        captureMicrophone = captureMic.toLowerCase() !== 'n';
        
        if (captureMicrophone) {
          // Choisir un microphone
          selectedMic = await chooseDevice(micDevices, 'microphone');
          if (selectedMic) {
            console.log(`\n✅ Microphone sélectionné: ${selectedMic.name} (ID=${selectedMic.id})`);
          }
        }
      }
    } catch (error) {
      console.warn(`⚠️ Capture microphone non disponible: ${error.message}`);
    }
    
    // Vérifier qu'au moins une source audio est sélectionnée
    if (!captureSystemAudio && !captureMicrophone) {
      console.error('❌ Erreur: Aucune source audio sélectionnée. Au moins une source est nécessaire.');
      rl.close();
      return;
    }
    
    // Demander la durée d'enregistrement
    const durationInput = await question(`\nDurée d'enregistrement en secondes (défaut: ${RECORDING_DURATION/1000}): `);
    const duration = durationInput === '' ? RECORDING_DURATION : parseInt(durationInput, 10) * 1000;
    
    if (isNaN(duration) || duration <= 0) {
      console.log(`Durée invalide, utilisation de la valeur par défaut: ${RECORDING_DURATION/1000} secondes`);
    }
    
    // Créer un enregistreur
    const recorder = createScreenRecorder();
    
    // Préparer les options
    const options = {
      // Écran requis même pour l'audio uniquement
      screenId: selectedScreen.id,
      // Audio
      audioDeviceId: selectedAudioDevice?.id,
      microphoneDeviceId: selectedMic?.id,
      // Option pour convertir automatiquement en MP3
      audioOnly: true,
      // Paramètres minimaux car on ne garde que l'audio
      fps: 1,
      showCursor: false,
      highlightClicks: false,
      cropArea: {
        x: 0,
        y: 0,
        width: 1,
        height: 1
      }
    };
    
    console.log('\nOptions d\'enregistrement:');
    console.log(JSON.stringify(options, null, 2));
    
    // Demander confirmation pour démarrer
    const startConfirm = await question('\nDémarrer l\'enregistrement audio? (O/n): ');
    
    if (startConfirm.toLowerCase() === 'n') {
      console.log('Enregistrement annulé.');
      rl.close();
      return;
    }
    
    // Démarrer l'enregistrement
    console.log('\nDémarrage de l\'enregistrement audio...');
    await recorder.startRecording(options);
    
    // Ouvrir YouTube dans le navigateur
    const youtubeURL = 'https://www.youtube.com/watch?v=xvFZjo5PgG0';
    await openYouTubeInBrowser(youtubeURL);
    
    console.log(`\nEnregistrement en cours pendant ${duration/1000} secondes...`);
    
    if (captureMicrophone) {
      console.log('Parlez dans votre microphone pour tester la capture audio!');
    }
    
    // Attendre la durée spécifiée
    await new Promise(resolve => setTimeout(resolve, duration));
    
    // Arrêter l'enregistrement
    console.log('Arrêt de l\'enregistrement...');
    const audioPath = await recorder.stopRecording();
    
    console.log(`\n✅ Audio enregistré à: ${audioPath}`);
    console.log('   L\'enregistrement contient:');
    console.log(`   - Audio système: ${captureSystemAudio ? '✅' : '❌'}`);
    console.log(`   - Audio microphone: ${captureMicrophone ? '✅' : '❌'}`);
    
    rl.close();
  } catch (error) {
    console.error('❌ Erreur lors de l\'enregistrement:', error);
    rl.close();
  }
}

main(); 

# ======================
# File: example/screen-with-audio.js
# ======================

// Example of screen capture with system audio
import createScreenRecorder from 'screencapturekit';
import { screens, audioDevices } from 'screencapturekit';
import readline from 'readline';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

const question = (query) => new Promise(resolve => rl.question(query, resolve));

async function main() {
  try {
    console.log('=== SCREEN CAPTURE WITH SYSTEM AUDIO ===');
    
    // Get available screens
    const availableScreens = await screens();
    if (!availableScreens || availableScreens.length === 0) {
      throw new Error('No screens available');
    }
    
    // Get audio devices
    const audioDeviceList = await audioDevices();
    if (!audioDeviceList || audioDeviceList.length === 0) {
      throw new Error('No system audio devices available');
    }
    
    // Display audio devices
    console.log('\nAvailable audio devices:');
    audioDeviceList.forEach((device, index) => {
      console.log(`[${index}] ${device.name} (${device.manufacturer || 'Unknown manufacturer'})`);
    });
    
    // Select an audio device
    const audioChoice = await question('\nChoose an audio device [0-' + (audioDeviceList.length - 1) + ']: ');
    const audioIndex = parseInt(audioChoice, 10);
    
    if (isNaN(audioIndex) || audioIndex < 0 || audioIndex >= audioDeviceList.length) {
      throw new Error('Invalid audio device selection');
    }
    
    const selectedAudio = audioDeviceList[audioIndex];
    console.log(`\nSelected audio device: ${selectedAudio.name}`);
    
    // Use the first screen
    const screen = availableScreens[0];
    console.log(`Using screen: ${screen.width}x${screen.height}`);
    
    // Create recorder
    const recorder = createScreenRecorder();
    
    // Capture options
    const options = {
      screenId: screen.id,
      audioDeviceId: selectedAudio.id,
      fps: 30,
      showCursor: true,
      highlightClicks: true
    };
    
    // Start recording
    console.log('\nStarting recording...');
    await recorder.startRecording(options);
    
    // Record for 15 seconds
    console.log('Recording in progress (15 seconds)...');
    console.log('Play some audio on your system to test it!');
    await new Promise(resolve => setTimeout(resolve, 15000));
    
    // Stop recording
    console.log('Stopping recording...');
    const videoPath = await recorder.stopRecording();
    
    console.log(`\n✅ Video saved to: ${videoPath}`);
    rl.close();
  } catch (error) {
    console.error('❌ Error:', error);
    rl.close();
  }
}

main(); 

# ======================
# File: example/index.js
# ======================

// Simple screen recording example
import createScreenRecorder from 'screencapturekit';
import { screens } from 'screencapturekit';
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __dirname = dirname(fileURLToPath(import.meta.url));

async function main() {
  try {
    console.log('=== SIMPLE SCREEN RECORDING EXAMPLE ===');
    
    // Get available screens
    const availableScreens = await screens();
    if (!availableScreens || availableScreens.length === 0) {
      throw new Error('No screens available');
    }
    
    // Use the first screen
    const screen = availableScreens[0];
    console.log(`Selected screen: ${screen.width}x${screen.height}`);
    
    // Create the recorder
    const recorder = createScreenRecorder();
    
    // Capture options
    const options = {
      screenId: screen.id,
      fps: 30,
      showCursor: true,
      highlightClicks: true
    };
    
    // Start recording
    console.log('\nStarting recording...');
    await recorder.startRecording(options);
    
    // Record for 5 seconds
    console.log('Recording in progress (5 seconds)...');
    await new Promise(resolve => setTimeout(resolve, 5000));
    
    // Stop recording
    console.log('Stopping recording...');
    const videoPath = await recorder.stopRecording();
    
    console.log(`\n✅ Video saved to: ${videoPath}`);
  } catch (error) {
    console.error('❌ Error:', error);
  }
}

main(); 

# ======================
# File: example/README.md
# ======================

# ScreenCaptureKit Examples for Node.js

This directory contains examples demonstrating the usage of the `screencapturekit` Node.js library, which enables screen and audio recording on macOS using Apple's native APIs.

## Prerequisites

- macOS 10.13 (High Sierra) or later
- Node.js 14 or later
- The `screencapturekit` library installed (will be used from the parent directory)

## Installation

To install example dependencies:

```bash
cd example
npm install
```

## Included Examples

### 1. Basic Screen Recording

A simple example demonstrating how to start and stop screen recording.

```bash
npm start
```

### 2. Screen Recording with System Audio

An example combining screen capture with system audio.

```bash
npm run screen-with-audio
```

### 3. Audio-Only Recording

An example of audio recording (system and/or microphone) controlled by keystrokes.

```bash
npm run audio-only
```

### 4. Advanced Example

A comprehensive example showcasing various features including:
- Specific display selection
- Screen region capture
- HDR support (if available)
- Custom configuration (FPS, cursor, click highlighting)

```bash
npm run advanced
```

### 5. List Available Devices

A utility to list all available displays, audio devices, and microphones.

```bash
npm run list-devices
```

## Demonstrated Features

- Basic screen capture
- Specific window capture
- Multi-display capture
- System audio capture
- Microphone capture (macOS 14+)
- Audio-only recording
- Specific display selection
- Region capture
- Cursor display and click highlighting options
- HDR support (on macOS 13+)
- Various video codecs
- Device enumeration

## Notes

- Recording files are saved in a temporary directory
- Microphone capture requires macOS 14 (Sonoma) or later
- HDR support requires macOS 13 (Ventura) or later
- System audio capture requires user authorization 

# ======================
# File: example/screen-only.js
# ======================

// Simple screen capture example
import createScreenRecorder from 'screencapturekit';
import { screens } from 'screencapturekit';
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __dirname = dirname(fileURLToPath(import.meta.url));

async function main() {
  try {
    console.log('=== SIMPLE SCREEN CAPTURE ===');
    
    // Get available screens
    const availableScreens = await screens();
    if (!availableScreens || availableScreens.length === 0) {
      throw new Error('No screens available');
    }
    
    // Use the first screen
    const screen = availableScreens[0];
    console.log(`Selected screen: ${screen.width}x${screen.height}`);
    
    // Create recorder
    const recorder = createScreenRecorder();
    
    // Capture options
    const options = {
      screenId: screen.id,
      fps: 30,
      showCursor: true,
      highlightClicks: true
    };
    
    // Start recording
    console.log('\nStarting recording...');
    await recorder.startRecording(options);
    
    // Record for 5 seconds
    await new Promise(resolve => setTimeout(resolve, 5000));
    
    // Stop recording
    console.log('Stopping recording...');
    const videoPath = await recorder.stopRecording();
    
    console.log(`\n✅ Video saved to: ${videoPath}`);
  } catch (error) {
    console.error('❌ Error:', error);
  }
}

main(); 

# ======================
# File: example/list-devices.js
# ======================

// Example of listing screens and audio devices
import { screens, audioDevices, microphoneDevices, supportsHDRCapture, videoCodecs } from 'screencapturekit';

async function main() {
  try {
    console.log('=== AVAILABLE DEVICES INFORMATION ===');
    
    // Check HDR support
    console.log(`\n🎨 HDR Support: ${supportsHDRCapture ? '✅ Supported' : '❌ Not supported'}`);
    
    // List available video codecs
    console.log('\n📹 Available Video Codecs:');
    console.log('--------------------');
    if (videoCodecs && videoCodecs.size > 0) {
      for (const [key, value] of videoCodecs.entries()) {
        console.log(`[${key}] ${value}`);
      }
    } else {
      console.log('No video codecs available');
    }
    
    // List available screens
    console.log('\n🖥️  Available Screens:');
    console.log('--------------------');
    const availableScreens = await screens();
    if (availableScreens && availableScreens.length > 0) {
      availableScreens.forEach((screen, index) => {
        console.log(`[${index}] ${screen.width}x${screen.height} (${screen.name || 'Unnamed'})`);
      });
    } else {
      console.log('No screens available');
    }
    
    // List system audio devices
    console.log('\n🔊 System Audio Devices:');
    console.log('--------------------');
    const systemAudio = await audioDevices();
    if (systemAudio && systemAudio.length > 0) {
      systemAudio.forEach((device, index) => {
        console.log(`[${index}] ${device.name} (${device.manufacturer || 'Unknown manufacturer'})`);
      });
    } else {
      console.log('No system audio devices available');
    }
    
    // List microphone devices (macOS 14+)
    console.log('\n🎤 Microphone Devices (macOS 14+):');
    console.log('--------------------');
    try {
      const mics = await microphoneDevices();
      if (mics && mics.length > 0) {
        mics.forEach((mic, index) => {
          console.log(`[${index}] ${mic.name} (${mic.manufacturer || 'Unknown manufacturer'})`);
        });
      } else {
        console.log('No microphones available');
      }
    } catch (error) {
      console.log(`❌ Not available: ${error.message}`);
    }
    
    console.log('\n✅ Device enumeration complete');
  } catch (error) {
    console.error('❌ Error:', error);
  }
}

main(); 

# ======================
# File: example/audio-only.js
# ======================

// Audio-only recording example with keyboard control
import createScreenRecorder from 'screencapturekit';
import { screens, audioDevices, microphoneDevices } from 'screencapturekit';
import readline from 'readline';

// Readline interface for interaction
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

// Utility function for questions
const question = (query) => new Promise(resolve => rl.question(query, resolve));

async function main() {
  try {
    console.log('=== AUDIO-ONLY RECORDING ===');
    
    // Get available screens (required for API)
    const availableScreens = await screens();
    if (!availableScreens || availableScreens.length === 0) {
      throw new Error('No screens available');
    }
    
    // Get audio devices
    const audioDeviceList = await audioDevices();
    const micDeviceList = await microphoneDevices();
    
    if ((!audioDeviceList || audioDeviceList.length === 0) && 
        (!micDeviceList || micDeviceList.length === 0)) {
      throw new Error('No audio devices available');
    }
    
    // Select audio devices
    let selectedAudioDevice = null;
    let selectedMicDevice = null;
    
    // System audio
    if (audioDeviceList && audioDeviceList.length > 0) {
      console.log('\n🔊 System Audio Devices:');
      console.log('--------------------');
      audioDeviceList.forEach((device, index) => {
        console.log(`[${index}] ${device.name} (${device.manufacturer || 'Unknown manufacturer'})`);
      });
      
      const useSystemAudio = await question('\nDo you want to capture system audio? (Y/n): ');
      if (useSystemAudio.toLowerCase() !== 'n') {
        const audioChoice = await question(`Choose a device [0-${audioDeviceList.length - 1}]: `);
        const audioIndex = parseInt(audioChoice, 10);
        
        if (!isNaN(audioIndex) && audioIndex >= 0 && audioIndex < audioDeviceList.length) {
          selectedAudioDevice = audioDeviceList[audioIndex];
          console.log(`✅ Selected system audio: ${selectedAudioDevice.name}`);
        }
      }
    }
    
    // Microphone
    if (micDeviceList && micDeviceList.length > 0) {
      console.log('\n🎤 Microphones:');
      console.log('--------------------');
      micDeviceList.forEach((mic, index) => {
        console.log(`[${index}] ${mic.name} (${mic.manufacturer || 'Unknown manufacturer'})`);
      });
      
      const useMicrophone = await question('\nDo you want to capture microphone? (Y/n): ');
      if (useMicrophone.toLowerCase() !== 'n') {
        const micChoice = await question(`Choose a microphone [0-${micDeviceList.length - 1}]: `);
        const micIndex = parseInt(micChoice, 10);
        
        if (!isNaN(micIndex) && micIndex >= 0 && micIndex < micDeviceList.length) {
          selectedMicDevice = micDeviceList[micIndex];
          console.log(`✅ Selected microphone: ${selectedMicDevice.name}`);
        }
      }
    }
    
    // Check that at least one source is selected
    if (!selectedAudioDevice && !selectedMicDevice) {
      throw new Error('No audio source selected');
    }
    
    // Create recorder
    const recorder = createScreenRecorder();
    
    // Basic options for audio-only recording
    const options = {
      screenId: availableScreens[0].id,
      audioDeviceId: selectedAudioDevice?.id,
      microphoneDeviceId: selectedMicDevice?.id,
      audioOnly: true,
      fps: 1,
      showCursor: false,
      highlightClicks: false,
      // Minimal area since we only keep audio
      cropArea: {
        x: 0,
        y: 0,
        width: 1,
        height: 1
      }
    };
    
    // Close readline interface and configure raw mode
    rl.close();
    process.stdin.setRawMode(true);
    process.stdin.resume();
    process.stdin.setEncoding('utf8');
    
    console.log('\nSetup complete:');
    console.log(`- System audio: ${selectedAudioDevice ? '✅' : '❌'}`);
    console.log(`- Microphone: ${selectedMicDevice ? '✅' : '❌'}`);
    
    console.log('\nPress:');
    console.log('- [s] to start/stop recording');
    console.log('- [q] to quit');
    
    let isRecording = false;
    
    // Key handler
    process.stdin.on('data', async (key) => {
      if (key === 's' && !isRecording) {
        isRecording = true;
        console.log('\n🔴 Recording started...');
        console.log('Press [s] to stop');
        await recorder.startRecording(options);
      } else if (key === 's' && isRecording) {
        isRecording = false;
        console.log('\n⏹️  Stopping recording...');
        const audioPath = await recorder.stopRecording();
        console.log(`\n✅ Audio saved to: ${audioPath}`);
      } else if (key === 'q') {
        if (isRecording) {
          console.log('\n⏹️  Stopping recording...');
          await recorder.stopRecording();
        }
        process.exit();
      }
    });
    
  } catch (error) {
    console.error('❌ Error:', error);
    process.exit(1);
  }
}

main(); 

# ======================
# File: Sources/screencapturekit-cli/Utilities.swift
# ======================

import Foundation

// MARK: - SignalHandler
struct SignalHandler {
  struct Signal: Hashable {
	static let hangup = Signal(rawValue: SIGHUP)
	static let interrupt = Signal(rawValue: SIGINT)
	static let quit = Signal(rawValue: SIGQUIT)
	static let abort = Signal(rawValue: SIGABRT)
	static let kill = Signal(rawValue: SIGKILL)
	static let alarm = Signal(rawValue: SIGALRM)
	static let termination = Signal(rawValue: SIGTERM)
	static let userDefined1 = Signal(rawValue: SIGUSR1)
	static let userDefined2 = Signal(rawValue: SIGUSR2)

	/**
	Signals that cause the process to exit.
	*/
	static let exitSignals = [
	  hangup,
	  interrupt,
	  quit,
	  abort,
	  alarm,
	  termination
	]

	let rawValue: Int32
	init(rawValue: Int32) {
	  self.rawValue = rawValue
	}
  }

  typealias CSignalHandler = @convention(c) (Int32) -> Void
  typealias SignalHandler = (Signal) -> Void

  private static var handlers = [Signal: [SignalHandler]]()

  private static var cHandler: CSignalHandler = { rawSignal in
	let signal = Signal(rawValue: rawSignal)

	guard let signalHandlers = handlers[signal] else {
	  return
	}

	for handler in signalHandlers {
	  handler(signal)
	}
  }

/**
Handle some signals
*/
  static func handle(signals: [Signal], handler: @escaping SignalHandler) {
	for signal in signals {
	  // Since Swift has no way of running code on "struct creation", we need to initialize here…
	  if handlers[signal] == nil {
		handlers[signal] = []
	  }
	  handlers[signal]?.append(handler)

	  var signalAction = sigaction(
		__sigaction_u: unsafeBitCast(cHandler, to: __sigaction_u.self),
		sa_mask: 0,
		sa_flags: 0
	  )

	  _ = withUnsafePointer(to: &signalAction) { pointer in
		sigaction(signal.rawValue, pointer, nil)
	  }
	}
  }

  /**
  Raise a signal.
  */
  static func raise(signal: Signal) {
	_ = Darwin.raise(signal.rawValue)
  }

  /**
  Ignore a signal.
  */
  static func ignore(signal: Signal) {
	_ = Darwin.signal(signal.rawValue, SIG_IGN)
  }

  /**
  Restore default signal handling.
  */
  static func restore(signal: Signal) {
	_ = Darwin.signal(signal.rawValue, SIG_DFL)
  }
}

extension Array where Element == SignalHandler.Signal {
  static let exitSignals = SignalHandler.Signal.exitSignals
}
// MARK: -


// MARK: - CLI utils
extension FileHandle: @retroactive TextOutputStream {
  public func write(_ string: String) {
	write(string.data(using: .utf8)!)
  }
}

enum CLI {
  static var standardInput = FileHandle.standardInput
  static var standardOutput = FileHandle.standardOutput
  static var standardError = FileHandle.standardError

  static let arguments = Array(CommandLine.arguments.dropFirst(1))
}

extension CLI {
  private static let once = Once()

  /**
  Called when the process exits, either normally or forced (through signals).

  When this is set, it's up to you to exit the process.
  */
  static var onExit: (() -> Void)? {
	didSet {
	  guard let exitHandler = onExit else {
		return
	  }

	  let handler = {
		once.run(exitHandler)
	  }

	  atexit_b {
		handler()
	  }

	  SignalHandler.handle(signals: .exitSignals) { _ in
		handler()
	  }
	}
  }

  /**
  Called when the process is being forced (through signals) to exit.

  When this is set, it's up to you to exit the process.
  */
  static var onForcedExit: ((SignalHandler.Signal) -> Void)? {
	didSet {
	  guard let exitHandler = onForcedExit else {
		return
	  }

	  SignalHandler.handle(signals: .exitSignals, handler: exitHandler)
	}
  }
}

enum PrintOutputTarget {
  case standardOutput
  case standardError
}

/**
Make `print()` accept an array of items.

Since Swift doesn't support spreading...
*/
private func print<Target>(
  _ items: [Any],
  separator: String = " ",
  terminator: String = "\n",
  to output: inout Target
) where Target: TextOutputStream {
  let item = items.map { "\($0)" }.joined(separator: separator)
  Swift.print(item, terminator: terminator, to: &output)
}

func print(
  _ items: Any...,
  separator: String = " ",
  terminator: String = "\n",
  to output: PrintOutputTarget = .standardOutput
) {
  switch output {
  case .standardOutput:
	print(items, separator: separator, terminator: terminator)
  case .standardError:
	print(items, separator: separator, terminator: terminator, to: &CLI.standardError)
  }
}
// MARK: -


// MARK: - Misc
func synchronized<T>(lock: AnyObject, closure: () throws -> T) rethrows -> T {
	objc_sync_enter(lock)
	defer {
		objc_sync_exit(lock)
	}

	return try closure()
}

final class Once {
  private var hasRun = false

  /**
  Executes the given closure only once (thread-safe)

  ```
  final class Foo {
	private let once = Once()

	func bar() {
	  once.run {
		print("Called only once")
	  }
	}
  }

  let foo = Foo()
  foo.bar()
  foo.bar()
  ```
  */
  func run(_ closure: () -> Void) {
	synchronized(lock: self) {
	  guard !hasRun else {
		return
	  }

	  hasRun = true
	  closure()
	}
  }
}

extension Data {
  func jsonDecoded<T: Decodable>() throws -> T {
	try JSONDecoder().decode(T.self, from: self)
  }
}

extension String {
  func jsonDecoded<T: Decodable>() throws -> T {
	try data(using: .utf8)!.jsonDecoded()
  }
}

func toJson<T>(_ data: T) throws -> String {
  let json = try JSONSerialization.data(withJSONObject: data)
  return String(data: json, encoding: .utf8)!
}
// MARK: -


# ======================
# File: Sources/screencapturekit-cli/ScreenCaptureKitCli.swift
# ======================

//
//  File.swift
//
//
//  Created by Mukesh Soni on 18/07/23.
//

// import AppKit
import ArgumentParser
import AVFoundation
import Foundation

import CoreGraphics
import ScreenCaptureKit

struct Options: Decodable {
    let destination: URL
    let framesPerSecond: Int
    let cropRect: CGRect?
    let showCursor: Bool
    let highlightClicks: Bool
    let screenId: CGDirectDisplayID
    let audioDeviceId: String?
    let microphoneDeviceId: String?
    let videoCodec: String?
    let enableHDR: Bool?
    let useDirectRecordingAPI: Bool?
}

@main
struct ScreenCaptureKitCLI: AsyncParsableCommand {
    static var configuration = CommandConfiguration(
        abstract: "Wrapper around ScreenCaptureKit",
        subcommands: [List.self, Record.self],
        defaultSubcommand: Record.self
    )
}

extension ScreenCaptureKitCLI {
    struct List: AsyncParsableCommand {
        static let configuration = CommandConfiguration(
            abstract: "List windows or screens which can be recorded",
            subcommands: [Screens.self, AudioDevices.self, MicrophoneDevices.self]
        )
    }

    struct Record: AsyncParsableCommand {
        static let configuration = CommandConfiguration(abstract: "Start a recording with the given options.")

        @Argument(help: "Stringified JSON object with options passed to ScreenCaptureKitCLI")
        var options: String

        mutating func run() async throws {
            var keepRunning = true
            let options: Options = try options.jsonDecoded()

            print(options)
            // Create a screen recording
            do {
                // Check for screen recording permission, make sure your terminal has screen recording permission
                guard CGPreflightScreenCaptureAccess() else {
                    throw RecordingError("No screen capture permission")
                }

                let screenRecorder = try await ScreenRecorder(
                    url: options.destination, 
                    displayID: options.screenId, 
                    showCursor: options.showCursor, 
                    cropRect: options.cropRect,
                    audioDeviceId: options.audioDeviceId,
                    microphoneDeviceId: options.microphoneDeviceId,
                    enableHDR: options.enableHDR ?? false,
                    useDirectRecordingAPI: options.useDirectRecordingAPI ?? false
                )
                
                print("Starting screen recording of display \(options.screenId)")
                try await screenRecorder.start()

                // Super duper hacky way to keep waiting for user's kill signal.
                // I have no idea if i am doing it right
                signal(SIGKILL, SIG_IGN)
                signal(SIGINT, SIG_IGN)
                signal(SIGTERM, SIG_IGN)
                let sigintSrc = DispatchSource.makeSignalSource(signal: SIGINT, queue: .main)
                sigintSrc.setEventHandler {
                    print("Got SIGINT")
                    keepRunning = false
                }
                sigintSrc.resume()
                let sigKillSrc = DispatchSource.makeSignalSource(signal: SIGKILL, queue: .main)
                sigKillSrc.setEventHandler {
                    print("Got SIGKILL")
                    keepRunning = false
                }
                sigKillSrc.resume()
                let sigTermSrc = DispatchSource.makeSignalSource(signal: SIGTERM, queue: .main)
                sigTermSrc.setEventHandler {
                    print("Got SIGTERM")
                    keepRunning = false
                }
                sigTermSrc.resume()

                // If i run the NSApplication run loop, then the mouse events are received
                // But i couldn't figure out a way to kill this run loop
                // Also, We have to import AppKit to run NSApplication run loop
                // await NSApplication.shared.run()
                // Keep looping and checking every 1 second if the user pressed the kill switch
                while true {
                    if !keepRunning {
                        try await screenRecorder.stop()
                        print("We are done. Have saved the recording to a file.")
                        break
                    } else {
                        sleep(1)
                    }
                }
            } catch {
                print("Error during recording:", error)
            }
        }
    }
}

extension ScreenCaptureKitCLI.List {
    struct Screens: AsyncParsableCommand {
        mutating func run() async throws {
            let sharableContent = try await SCShareableContent.current
            print(sharableContent.displays.count, sharableContent.windows.count, sharableContent.applications.count)
            let screens = sharableContent.displays.map { display in
                ["id": display.displayID, "width": display.width, "height": display.height]
            }
            try print(toJson(screens), to: .standardError)
        }
    }
    
    struct AudioDevices: AsyncParsableCommand {
        mutating func run() async throws {
            let discoverySession = AVCaptureDevice.DiscoverySession(
                deviceTypes: [.builtInMicrophone, .externalUnknown],
                mediaType: .audio,
                position: .unspecified
            )
            let devices = discoverySession.devices
            let audioDevices = devices.map { device in
                ["id": device.uniqueID, "name": device.localizedName, "manufacturer": device.manufacturer]
            }
            try print(toJson(audioDevices), to: .standardError)
        }
    }
    
    struct MicrophoneDevices: AsyncParsableCommand {
        mutating func run() async throws {
            let discoverySession = AVCaptureDevice.DiscoverySession(
                deviceTypes: [.builtInMicrophone, .externalUnknown],
                mediaType: .audio,
                position: .unspecified
            )
            let devices = discoverySession.devices.filter { $0.hasMediaType(.audio) }
            let microphones = devices.map { device in
                ["id": device.uniqueID, "name": device.localizedName, "manufacturer": device.manufacturer]
            }
            try print(toJson(microphones), to: .standardError)
        }
    }
}

@available(macOS, introduced: 10.13)
struct ScreenRecorder {
    private let videoSampleBufferQueue = DispatchQueue(label: "ScreenRecorder.VideoSampleBufferQueue")
    private let audioSampleBufferQueue = DispatchQueue(label: "ScreenRecorder.AudioSampleBufferQueue")
    private let microphoneSampleBufferQueue = DispatchQueue(label: "ScreenRecorder.MicrophoneSampleBufferQueue")

    private let assetWriter: AVAssetWriter
    private let videoInput: AVAssetWriterInput
    private var audioInput: AVAssetWriterInput?
    private var microphoneInput: AVAssetWriterInput?
    private let streamOutput: StreamOutput
    private var stream: SCStream
    
    private var _recordingOutput: Any?
    
    private var useDirectRecording: Bool

    init(
        url: URL, 
        displayID: CGDirectDisplayID, 
        showCursor: Bool = true, 
        cropRect: CGRect? = nil,
        audioDeviceId: String? = nil,
        microphoneDeviceId: String? = nil,
        enableHDR: Bool = false,
        useDirectRecordingAPI: Bool = false
    ) async throws {
        self.useDirectRecording = useDirectRecordingAPI
        
        // Create AVAssetWriter for a QuickTime movie file
        assetWriter = try AVAssetWriter(url: url, fileType: .mov)

        // MARK: AVAssetWriter setup

        // Get size and pixel scale factor for display
        let displaySize = CGDisplayBounds(displayID).size

        // The number of physical pixels that represent a logic point on screen
        let displayScaleFactor: Int
        if let mode = CGDisplayCopyDisplayMode(displayID) {
            displayScaleFactor = mode.pixelWidth / mode.width
        } else {
            displayScaleFactor = 1
        }

        // AVAssetWriterInput supports maximum resolution of 4096x2304 for H.264
        let videoSize = downsizedVideoSize(source: cropRect?.size ?? displaySize, scaleFactor: displayScaleFactor)

        // Utiliser le preset 4K maximal
        guard let assistant = AVOutputSettingsAssistant(preset: .preset3840x2160) else {
            throw RecordingError("Can't create AVOutputSettingsAssistant with .preset3840x2160")
        }
        assistant.sourceVideoFormat = try CMVideoFormatDescription(videoCodecType: .h264, width: videoSize.width, height: videoSize.height)

        guard var outputSettings = assistant.videoSettings else {
            throw RecordingError("AVOutputSettingsAssistant has no videoSettings")
        }
        outputSettings[AVVideoWidthKey] = videoSize.width
        outputSettings[AVVideoHeightKey] = videoSize.height
        
        // Configure HDR settings if enabled
        if enableHDR {
            if #available(macOS 13.0, *) {
                outputSettings[AVVideoColorPropertiesKey] = [
                    AVVideoColorPrimariesKey: AVVideoColorPrimaries_ITU_R_2020,
                    AVVideoTransferFunctionKey: AVVideoTransferFunction_ITU_R_2100_HLG,
                    AVVideoYCbCrMatrixKey: AVVideoYCbCrMatrix_ITU_R_2020
                ]
            } else {
                print("HDR requested but not supported on this macOS version")
            }
        }

        // Create AVAssetWriter input for video
        videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: outputSettings)
        videoInput.expectsMediaDataInRealTime = true
        
        // Configure audio input if an audio device is specified
        if audioDeviceId != nil {
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 48000,
                AVNumberOfChannelsKey: 2,
                AVEncoderBitRateKey: 256000
            ]
            
            audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioInput?.expectsMediaDataInRealTime = true
            
            if let audioInput = audioInput, assetWriter.canAdd(audioInput) {
                assetWriter.add(audioInput)
            }
        }
        
        // Configure microphone input if a microphone device is specified
        if microphoneDeviceId != nil {
            let micSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 48000,
                AVNumberOfChannelsKey: 1,
                AVEncoderBitRateKey: 128000
            ]
            
            microphoneInput = AVAssetWriterInput(mediaType: .audio, outputSettings: micSettings)
            microphoneInput?.expectsMediaDataInRealTime = true
            
            if let microphoneInput = microphoneInput, assetWriter.canAdd(microphoneInput) {
                assetWriter.add(microphoneInput)
            }
        }
        
        streamOutput = StreamOutput(
            videoInput: videoInput,
            audioInput: audioInput,
            microphoneInput: microphoneInput
        )

        // Adding videoInput to assetWriter
        guard assetWriter.canAdd(videoInput) else {
            throw RecordingError("Can't add input to asset writer")
        }
        assetWriter.add(videoInput)

        guard assetWriter.startWriting() else {
            if let error = assetWriter.error {
                throw error
            }
            throw RecordingError("Couldn't start writing to AVAssetWriter")
        }

        // MARK: SCStream setup

        // Obtenir le contenu partageable
        let sharableContent = try await SCShareableContent.current
        print("Displays: \(sharableContent.displays.count), Windows: \(sharableContent.windows.count), Apps: \(sharableContent.applications.count)")
        
        // Trouver l'écran demandé
        guard let display = sharableContent.displays.first(where: { $0.displayID == displayID }) else {
            throw RecordingError("No display with ID \(displayID) found")
        }
        
        let filter = SCContentFilter(display: display, excludingWindows: [])
        
        // Configurer le stream
        var config: SCStreamConfiguration
        
        if enableHDR, #available(macOS 13.0, *) {
            // Pour macOS 15+, utiliser le preset HDR
            if #available(macOS 15.0, *) {
                let preset = SCStreamConfiguration.Preset.captureHDRStreamCanonicalDisplay
                config = SCStreamConfiguration(preset: preset)
            } else {
                // Fallback pour macOS 13-14
                config = SCStreamConfiguration()
                // Pour macOS 13-14, nous n'avons pas de méthode simple pour activer HDR
                // sans utiliser d'API dépréciée
                print("HDR enabled but limited support on this macOS version")
            }
        } else {
            config = SCStreamConfiguration()
        }
        
        // Configurer la fréquence d'images
        config.minimumFrameInterval = CMTime(value: 1, timescale: Int32(truncating: NSNumber(value: showCursor ? 60 : 30)))
        config.showsCursor = showCursor
        
        // Configurer la capture du son système si nécessaire
        if let _ = audioDeviceId {
            config.capturesAudio = true
            config.excludesCurrentProcessAudio = true
            print("System audio capture enabled")
        }
        
        // Configurer la capture de microphone si nécessaire
        if let microphoneDeviceId = microphoneDeviceId {
            if #available(macOS 15.0, *) {
                config.captureMicrophone = true
                config.microphoneCaptureDeviceID = microphoneDeviceId
            } else {
                print("Microphone capture with direct API requires macOS 15.0+")
            }
        }
        
        // Créer le stream
        stream = SCStream(filter: filter, configuration: config, delegate: nil)
        
        // Utiliser l'API d'enregistrement direct si spécifié
        if useDirectRecordingAPI {
            if #available(macOS 15.0, *) {
                let recordingConfig = SCRecordingOutputConfiguration()
                recordingConfig.outputURL = url
                
                let recordingDelegate = RecordingDelegate()
                let recOutput = SCRecordingOutput(configuration: recordingConfig, delegate: recordingDelegate)
                _recordingOutput = recOutput
                
                do {
                    try stream.addRecordingOutput(recOutput)
                } catch {
                    throw RecordingError("Failed to add recording output: \(error)")
                }
            } else {
                print("Direct recording API requires macOS 15.0+, falling back to manual recording")
                self.useDirectRecording = false
            }
        }
        
        // Configuration de sortie de stream pour l'enregistrement manuel
        if !useDirectRecordingAPI || !self.useDirectRecording {
            try stream.addStreamOutput(streamOutput, type: .screen, sampleHandlerQueue: videoSampleBufferQueue)
            
            if audioDeviceId != nil {
                try stream.addStreamOutput(streamOutput, type: .audio, sampleHandlerQueue: audioSampleBufferQueue)
            }
            
            if microphoneDeviceId != nil {
                if #available(macOS 15.0, *) {
                    try stream.addStreamOutput(streamOutput, type: .microphone, sampleHandlerQueue: microphoneSampleBufferQueue)
                } else {
                    print("Microphone stream output requires macOS 15.0+, skipping")
                }
            }
        }
    }

    func start() async throws {
        // Start capturing, wait for stream to start
        try await stream.startCapture()

        // Start the AVAssetWriter session at source time .zero, sample buffers will need to be re-timed
        assetWriter.startSession(atSourceTime: .zero)
        streamOutput.sessionStarted = true
    }

    func stop() async throws {
        // Stop capturing, wait for stream to stop
        try await stream.stopCapture()

        // Repeat the last frame and add it at the current time
        // In case no changes happend on screen, and the last frame is from long ago
        // This ensures the recording is of the expected length
        if let originalBuffer = streamOutput.lastSampleBuffer {
            let additionalTime = CMTime(seconds: ProcessInfo.processInfo.systemUptime, preferredTimescale: 100) - streamOutput.firstSampleTime
            let timing = CMSampleTimingInfo(duration: originalBuffer.duration, presentationTimeStamp: additionalTime, decodeTimeStamp: originalBuffer.decodeTimeStamp)
            let additionalSampleBuffer = try CMSampleBuffer(copying: originalBuffer, withNewTiming: [timing])
            videoInput.append(additionalSampleBuffer)
            streamOutput.lastSampleBuffer = additionalSampleBuffer
        }

        // Stop the AVAssetWriter session at time of the repeated frame
        assetWriter.endSession(atSourceTime: streamOutput.lastSampleBuffer?.presentationTimeStamp ?? .zero)

        // Finish writing
        videoInput.markAsFinished()
        audioInput?.markAsFinished()
        microphoneInput?.markAsFinished()
        
        await assetWriter.finishWriting()
    }

    private class StreamOutput: NSObject, SCStreamOutput {
        let videoInput: AVAssetWriterInput
        let audioInput: AVAssetWriterInput?
        let microphoneInput: AVAssetWriterInput?
        
        var sessionStarted = false
        var firstSampleTime: CMTime = .zero
        var lastSampleBuffer: CMSampleBuffer?

        init(videoInput: AVAssetWriterInput, 
             audioInput: AVAssetWriterInput? = nil, 
             microphoneInput: AVAssetWriterInput? = nil) {
            self.videoInput = videoInput
            self.audioInput = audioInput
            self.microphoneInput = microphoneInput
        }

        func stream(_: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
            // Return early if session hasn't started yet
            guard sessionStarted else { return }

            // Return early if the sample buffer is invalid
            guard sampleBuffer.isValid else { return }

            switch type {
            case .screen:
                handleVideoSampleBuffer(sampleBuffer)
            case .audio:
                handleAudioSampleBuffer(sampleBuffer, isFromMicrophone: false)
            case .microphone:
                handleAudioSampleBuffer(sampleBuffer, isFromMicrophone: true)
            @unknown default:
                break
            }
        }
        
        private func handleVideoSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
            guard videoInput.isReadyForMoreMediaData else {
                print("AVAssetWriterInput (video) isn't ready, dropping frame")
                return
            }
            
            // Retrieve the array of metadata attachments from the sample buffer
            guard let attachmentsArray = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, createIfNecessary: false) as? [[SCStreamFrameInfo: Any]],
                  let attachments = attachmentsArray.first
            else { return }

            // Validate the status of the frame. If it isn't `.complete`, return
            guard let statusRawValue = attachments[SCStreamFrameInfo.status] as? Int,
                  let status = SCFrameStatus(rawValue: statusRawValue),
                  status == .complete
            else { return }
            
            // Save the timestamp of the current sample, all future samples will be offset by this
            if firstSampleTime == .zero {
                firstSampleTime = sampleBuffer.presentationTimeStamp
            }

            // Offset the time of the sample buffer, relative to the first sample
            let lastSampleTime = sampleBuffer.presentationTimeStamp - firstSampleTime

            // Always save the last sample buffer.
            // This is used to "fill up" empty space at the end of the recording.
            //
            // Note that this permanently captures one of the sample buffers
            // from the ScreenCaptureKit queue.
            // Make sure reserve enough in SCStreamConfiguration.queueDepth
            lastSampleBuffer = sampleBuffer

            // Create a new CMSampleBuffer by copying the original, and applying the new presentationTimeStamp
            let timing = CMSampleTimingInfo(duration: sampleBuffer.duration, presentationTimeStamp: lastSampleTime, decodeTimeStamp: sampleBuffer.decodeTimeStamp)
            if let retimedSampleBuffer = try? CMSampleBuffer(copying: sampleBuffer, withNewTiming: [timing]) {
                videoInput.append(retimedSampleBuffer)
            } else {
                print("Couldn't copy CMSampleBuffer, dropping frame")
            }
        }
        
        private func handleAudioSampleBuffer(_ sampleBuffer: CMSampleBuffer, isFromMicrophone: Bool) {
            let input = isFromMicrophone ? microphoneInput : audioInput
            
            guard let audioInput = input, audioInput.isReadyForMoreMediaData else {
                if input != nil {
                    print("AVAssetWriterInput (audio) isn't ready, dropping sample")
                }
                return
            }
            
            // Offset audio sample relative to video start time
            if firstSampleTime == .zero {
                // If first video sample hasn't arrived yet, cache this audio sample for later
                return
            }
            
            // Retime audio sample buffer to match video timeline
            let presentationTime = sampleBuffer.presentationTimeStamp - firstSampleTime
            let timing = CMSampleTimingInfo(
                duration: sampleBuffer.duration,
                presentationTimeStamp: presentationTime,
                decodeTimeStamp: .invalid
            )
            
            if let retimedSampleBuffer = try? CMSampleBuffer(copying: sampleBuffer, withNewTiming: [timing]) {
                audioInput.append(retimedSampleBuffer)
            } else {
                print("Couldn't copy audio CMSampleBuffer, dropping sample")
            }
        }
    }
}

// AVAssetWriterInput supports maximum resolution of 4096x2304 for H.264
private func downsizedVideoSize(source: CGSize, scaleFactor: Int) -> (width: Int, height: Int) {
    let maxSize = CGSize(width: 4096, height: 2304)

    let w = source.width * Double(scaleFactor)
    let h = source.height * Double(scaleFactor)
    let r = max(w / maxSize.width, h / maxSize.height)

    return r > 1
        ? (width: Int(w / r), height: Int(h / r))
        : (width: Int(w), height: Int(h))
}

struct RecordingError: Error, CustomDebugStringConvertible {
    var debugDescription: String
    init(_ debugDescription: String) { self.debugDescription = debugDescription }
}

// Add required delegate for direct recording
@available(macOS 15.0, *)
class RecordingDelegate: NSObject, SCRecordingOutputDelegate {
    func recordingOutput(_ output: SCRecordingOutput, didStartRecordingWithError error: Error?) {
        if let error = error {
            print("Recording started with error: \(error)")
        } else {
            print("Recording started successfully")
        }
    }
    
    func recordingOutput(_ output: SCRecordingOutput, didFinishRecordingWithError error: Error?) {
        if let error = error {
            print("Recording finished with error: \(error)")
        } else {
            print("Recording finished successfully")
        }
    }
}

extension AVCaptureDevice {
    var manufacturer: String {
        // La méthode properties n'existe pas
        // Utilisons une valeur par défaut
        return "Unknown"
    }
}


# ======================
# File: .vscode/launch.json
# ======================

{
  "configurations": [
    {
      "type": "lldb",
      "request": "launch",
      "sourceLanguages": ["swift"],
      "name": "Debug screencapturekit-cli",
      "program": "${workspaceFolder:screencapturekit-node}/.build/debug/screencapturekit-cli",
      "args": [],
      "cwd": "${workspaceFolder:screencapturekit-node}",
      "preLaunchTask": "swift: Build Debug screencapturekit-cli"
    },
    {
      "type": "lldb",
      "request": "launch",
      "sourceLanguages": ["swift"],
      "name": "Release screencapturekit-cli",
      "program": "${workspaceFolder:screencapturekit-node}/.build/release/screencapturekit-cli",
      "args": [],
      "cwd": "${workspaceFolder:screencapturekit-node}",
      "preLaunchTask": "swift: Build Release screencapturekit-cli"
    },
    {
      "type": "lldb",
      "request": "launch",
      "sourceLanguages": ["swift"],
      "name": "Debug screencapturekit",
      "program": "${workspaceFolder:screencapturekit-node}/.build/debug/screencapturekit",
      "args": [],
      "cwd": "${workspaceFolder:screencapturekit-node}",
      "preLaunchTask": "swift: Build Debug screencapturekit"
    },
    {
      "type": "lldb",
      "request": "launch",
      "sourceLanguages": ["swift"],
      "name": "Release screencapturekit",
      "program": "${workspaceFolder:screencapturekit-node}/.build/release/screencapturekit",
      "args": [],
      "cwd": "${workspaceFolder:screencapturekit-node}",
      "preLaunchTask": "swift: Build Release screencapturekit"
    }
  ]
}


# ======================
# File: src/index.test.js
# ======================

import fs from "node:fs";
import path from "node:path";
import { test, expect, afterEach, beforeEach, vi } from "vitest";
// import test from "ava";
import delay from "delay";
import { readChunkSync } from "read-chunk";
import { fileTypeFromBuffer } from "file-type";
import sck, { videoCodecs, screens, audioDevices, microphoneDevices, supportsHDRCapture } from "./index.ts";

const TEST_TIMEOUT = 15000;
const RECORDING_DURATION = 3000;
let videoPath;
let recorder;

beforeEach(() => {
  recorder = sck();
});

afterEach(async () => {
  // Make sure recording is stopped
  if (recorder) {
    try {
      const stopResult = await recorder.stopRecording().catch(() => null);
      if (stopResult) videoPath = stopResult;
    } catch (err) {
      // Ignore errors if recording wasn't started
    }
  }

  // Clean up video file
  if (videoPath && fs.existsSync(videoPath)) {
    try {
      fs.unlinkSync(videoPath);
    } catch (err) {
      console.warn(`Failed to delete file: ${videoPath}`, err);
    }
    videoPath = undefined;
  }
});

test("returns available codecs", () => {
  expect(videoCodecs).toBeDefined();
  expect(videoCodecs instanceof Map).toBe(true);
  expect(videoCodecs.size).toBeGreaterThan(0);
  expect(videoCodecs.has("h264")).toBe(true);
});

test("records screen correctly", async () => {
  // First get available screens to use a valid ID
  const screenList = await screens();
  expect(screenList).toBeDefined();
  expect(Array.isArray(screenList)).toBe(true);
  
  // Skip test if no screens available
  if (screenList.length === 0) {
    console.log("Test skipped - No screens available");
    return;
  }
  
  // Use the first available screen
  const screenId = screenList[0].id;
  
  // Start recording with a valid screen ID
  await expect(recorder.startRecording({
    screenId
  })).resolves.not.toThrow();
  
  // Wait for recording duration
  await delay(RECORDING_DURATION);
  
  // Stop recording and get the path
  videoPath = await recorder.stopRecording();
  
  // Check that the file was created
  expect(videoPath).toBeDefined();
  expect(typeof videoPath).toBe("string");
  expect(fs.existsSync(videoPath)).toBe(true);
  
  // Check that the file is not empty
  const stats = fs.statSync(videoPath);
  if (stats.size === 0) {
    console.warn("Video file exists but has size 0. Recording may have silently failed.");
  }
  
  try {
    // Try to read the file metadata
    const fileBuffer = readChunkSync(videoPath, { startPosition: 0, length: 4100 });
    const fileInfo = await fileTypeFromBuffer(fileBuffer);
    
    // Verify format only if recording succeeded
    if (fileInfo) {
      expect(fileInfo.ext).toBe("mov");
      expect(fileInfo.mime).toBe("video/quicktime");
    }
  } catch (error) {
    console.warn("Unable to read file metadata:", error.message);
  }
}, TEST_TIMEOUT);

test("lists available screens", async () => {
  const screenList = await screens();
  expect(screenList).toBeDefined();
  expect(Array.isArray(screenList)).toBe(true);
  expect(screenList.length).toBeGreaterThan(0);
  
  // Check that each screen has the expected properties
  screenList.forEach(screen => {
    expect(screen).toHaveProperty("id");
    expect(screen).toHaveProperty("width");
    expect(screen).toHaveProperty("height");
    // Note: "name" property is not available in the current implementation
  });
});

test("lists audio devices", async () => {
  const deviceList = await audioDevices();
  expect(deviceList).toBeDefined();
  expect(Array.isArray(deviceList)).toBe(true);
  
  // If devices are present, verify their properties
  if (deviceList.length > 0) {
    deviceList.forEach(device => {
      expect(device).toHaveProperty("id");
      expect(device).toHaveProperty("name");
      expect(device).toHaveProperty("manufacturer");
    });
  }
});

test("lists microphone devices", async () => {
  const micList = await microphoneDevices();
  expect(micList).toBeDefined();
  expect(Array.isArray(micList)).toBe(true);
  
  // If microphones are present, verify their properties
  if (micList.length > 0) {
    micList.forEach(mic => {
      expect(mic).toHaveProperty("id");
      expect(mic).toHaveProperty("name");
      expect(mic).toHaveProperty("manufacturer");
    });
  }
});

test("records with custom options", async () => {
  // First get available screens to use a valid ID
  const screenList = await screens();
  
  // Skip test if no screens available
  if (screenList.length === 0) {
    console.log("Test skipped - No screens available");
    return;
  }
  
  // Use the first available screen
  const screenId = screenList[0].id;
  
  const options = {
    fps: 30,
    showCursor: true,
    highlightClicks: true,
    videoCodec: "h264",
    screenId
  };
  
  await expect(recorder.startRecording(options)).resolves.not.toThrow();
  await delay(RECORDING_DURATION);
  
  videoPath = await recorder.stopRecording();
  expect(videoPath).toBeDefined();
  expect(fs.existsSync(videoPath)).toBe(true);
  
  // File size could be 0 in case of silent failure
  // We only check for file existence
  const stats = fs.statSync(videoPath);
  if (stats.size === 0) {
    console.warn("Video file exists but has size 0. Recording may have silently failed.");
  }
}, TEST_TIMEOUT);

test("correctly indicates HDR capability", () => {
  expect(supportsHDRCapture).toBeDefined();
  expect(typeof supportsHDRCapture).toBe("boolean");
});

test("records with HDR if supported", async () => {
  // Skip test if HDR is not supported
  if (!supportsHDRCapture) {
    console.log("Test skipped - HDR not supported on this system");
    return;
  }
  
  // First get available screens to use a valid ID
  const screenList = await screens();
  
  // Skip test if no screens available
  if (screenList.length === 0) {
    console.log("Test skipped - No screens available");
    return;
  }
  
  // Use the first available screen
  const screenId = screenList[0].id;
  
  await expect(recorder.startRecording({
    enableHDR: true,
    showCursor: true,
    screenId
  })).resolves.not.toThrow();
  
  await delay(RECORDING_DURATION);
  
  videoPath = await recorder.stopRecording();
  expect(videoPath).toBeDefined();
  expect(fs.existsSync(videoPath)).toBe(true);
  
  // File size could be 0 in case of silent failure
  const stats = fs.statSync(videoPath);
  if (stats.size === 0) {
    console.warn("Video file exists but has size 0. Recording may have silently failed.");
  }
}, TEST_TIMEOUT);

test("records directly to file if supported", async () => {
  // First get available screens to use a valid ID
  const screenList = await screens();
  
  // Skip test if no screens available
  if (screenList.length === 0) {
    console.log("Test skipped - No screens available");
    return;
  }
  
  // Use the first available screen
  const screenId = screenList[0].id;
  
  await expect(recorder.startRecording({
    recordToFile: true,
    showCursor: true,
    screenId
  })).resolves.not.toThrow();
  
  await delay(RECORDING_DURATION);
  
  videoPath = await recorder.stopRecording();
  expect(videoPath).toBeDefined();
  expect(fs.existsSync(videoPath)).toBe(true);
  
  // File size could be 0 in case of silent failure
  const stats = fs.statSync(videoPath);
  if (stats.size === 0) {
    console.warn("Video file exists but has size 0. Recording may have silently failed.");
  }
}, TEST_TIMEOUT);

test("handles recording errors with invalid codec", async () => {
  await expect(
    recorder.startRecording({ videoCodec: "codec_inexistant" })
  ).rejects.toThrow();
});

test("handles recording errors with invalid screen ID", async () => {
  // Use a high ID that probably doesn't exist
  const invalidScreenId = 99999;
  
  await expect(
    recorder.startRecording({ screenId: invalidScreenId })
  ).rejects.toThrow();
});


# ======================
# File: src/index.ts
# ======================

import os from "node:os";
import path from "node:path";
import { temporaryFile } from "tempy";
import * as macosVersion from "macos-version";
import fileUrl from "file-url";
// import { fixPathForAsarUnpack } from "electron-util";
import { execa, type ExecaChildProcess } from "execa";
import { fileURLToPath } from "url";
import fs from "node:fs/promises";

declare var __dirname_cjs_shim: string;

// Gestion du chemin compatible ESM et CJS
let __dirname;
if (typeof import.meta.url !== 'undefined') {
    __dirname = path.dirname(fileURLToPath(import.meta.url));
} else {
    __dirname = process.cwd(); // Fallback vers le répertoire de travail actuel
}
const BIN = path.join(__dirname, "../dist/screencapturekit");

/**
 * Generates a random identifier composed of alphanumeric characters.
 * @returns {string} A random identifier as a string.
 * @private
 */
const getRandomId = () => Math.random().toString(36).slice(2, 15);

/**
 * Checks if the system supports HEVC (H.265) hardware encoding.
 * @returns {boolean} True if the system supports HEVC hardware encoding, false otherwise.
 * @private
 */
const supportsHevcHardwareEncoding = (() => {
  const cpuModel = os.cpus()[0].model;

  // All Apple silicon Macs support HEVC hardware encoding.
  if (cpuModel.startsWith("Apple ")) {
    // Source string example: 'Apple M1'
    return true;
  }

  // Get the Intel Core generation, the `4` in `Intel(R) Core(TM) i7-4850HQ CPU @ 2.30GHz`
  // More info: https://www.intel.com/content/www/us/en/processors/processor-numbers.html
  // Example strings:
  // - `Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz`
  // - `Intel(R) Core(TM) i7-4850HQ CPU @ 2.30GHz`
  const result = /Intel.*Core.*i\d+-(\d)/.exec(cpuModel);

  // Intel Core generation 6 or higher supports HEVC hardware encoding
  return result && Number.parseInt(result[1], 10) >= 6;
})();

/**
 * Checks if the system supports HDR capture.
 * @returns {boolean} True if the system supports HDR capture (macOS 13.0+), false otherwise.
 * @private
 */
const supportsHDR = (() => {
  return macosVersion.isMacOSVersionGreaterThanOrEqualTo("13.0"); // HDR requires macOS 13.0+ (Ventura)
})();

/**
 * Checks if the system supports the direct recording API.
 * @returns {boolean} True if the system supports direct recording API (macOS 15.0+), false otherwise.
 * @private
 */
const supportsDirectRecordingAPI = (() => {
  return macosVersion.isMacOSVersionGreaterThanOrEqualTo("15.0"); // Direct Recording API requires macOS 15.0+
})();

/**
 * Checks if the system supports microphone capture.
 * @returns {boolean} True if the system supports microphone capture (macOS 15.0+), false otherwise.
 * @private
 */
const supportsMicrophoneCapture = (() => {
  return macosVersion.isMacOSVersionGreaterThanOrEqualTo("15.0"); // Microphone support with SCStream requires macOS 15.0+
})();

/**
 * Interface defining a cropping area for recording.
 * @typedef {Object} CropArea
 * @property {number} x - The X position of the starting point of the area.
 * @property {number} y - The Y position of the starting point of the area.
 * @property {number} width - The width of the area to capture.
 * @property {number} height - The height of the area to capture.
 */
type CropArea = {
  x: number;
  y: number;
  width: number;
  height: number;
};

/**
 * Options for screen recording.
 * @typedef {Object} RecordingOptions
 * @property {number} fps - Frames per second.
 * @property {CropArea} [cropArea] - Area of the screen to capture.
 * @property {boolean} showCursor - Show the cursor in the recording.
 * @property {boolean} highlightClicks - Highlight mouse clicks.
 * @property {number} screenId - Identifier of the screen to capture.
 * @property {number} [audioDeviceId] - Identifier of the system audio device.
 * @property {string} [microphoneDeviceId] - Identifier of the microphone device.
 * @property {string} videoCodec - Video codec to use.
 * @property {boolean} [enableHDR] - Enable HDR recording (on macOS 13.0+).
 * @property {boolean} [recordToFile] - Use the direct recording API (on macOS 14.0+).
 * @property {boolean} [audioOnly] - Record audio only, will convert to mp3 after recording.
 */
type RecordingOptions = {
  fps: number;
  cropArea?: CropArea;
  showCursor: boolean;
  highlightClicks: boolean;
  screenId: number;
  audioDeviceId?: number;
  microphoneDeviceId?: string; // Added support for microphone capture
  videoCodec: string;
  enableHDR?: boolean; // Added support for HDR
  recordToFile?: boolean; // Added support for direct file recording
  audioOnly?: boolean; // Added support for audio only recording
};

/**
 * Internal options for recording with ScreenCaptureKit.
 * @typedef {Object} RecordingOptionsForScreenCaptureKit
 * @property {string} destination - URL of the destination file.
 * @property {number} framesPerSecond - Frames per second.
 * @property {boolean} showCursor - Show the cursor in the recording.
 * @property {boolean} highlightClicks - Highlight mouse clicks.
 * @property {number} screenId - Identifier of the screen to capture.
 * @property {number} [audioDeviceId] - Identifier of the system audio device.
 * @property {string} [microphoneDeviceId] - Identifier of the microphone device.
 * @property {string} [videoCodec] - Video codec to use.
 * @property {Array} [cropRect] - Coordinates of the cropping area.
 * @property {boolean} [enableHDR] - Enable HDR recording.
 * @property {boolean} [useDirectRecordingAPI] - Use the direct recording API.
 * @private
 */
type RecordingOptionsForScreenCaptureKit = {
  destination: string;
  framesPerSecond: number;
  showCursor: boolean;
  highlightClicks: boolean;
  screenId: number;
  audioDeviceId?: number;
  microphoneDeviceId?: string; // Added support for microphone
  videoCodec?: string;
  cropRect?: [[x: number, y: number], [width: number, height: number]];
  enableHDR?: boolean; // Added support for HDR
  useDirectRecordingAPI?: boolean; // Use new recording API
};

/**
 * Main class for screen recording with ScreenCaptureKit.
 * Allows capturing the screen using Apple's native APIs.
 */
class ScreenCaptureKit {
  /** Path to the output video file. */
  videoPath: string | null = null;
  /** The ongoing recording process. */
  recorder?: ExecaChildProcess;
  /** Unique identifier of the recording process. */
  processId: string | null = null;
  /** Options used for recording */
  private currentOptions?: Partial<RecordingOptions>;
  /** Path to the final processed video file */
  processedVideoPath: string | null = null;

  /**
   * Creates a new instance of ScreenCaptureKit.
   * Checks that the macOS version is compatible (10.13+).
   * @throws {Error} If the macOS version is not supported.
   */
  constructor() {
    macosVersion.assertMacOSVersionGreaterThanOrEqualTo("10.13");
  }

  /**
   * Checks that recording has been started.
   * @throws {Error} If recording has not been started.
   * @private
   */
  throwIfNotStarted() {
    if (this.recorder === undefined) {
      throw new Error("Call `.startRecording()` first");
    }
  }

  /**
   * Starts screen recording.
   * @param {Partial<RecordingOptions>} options - Recording options.
   * @param {number} [options.fps=30] - Frames per second.
   * @param {CropArea} [options.cropArea] - Area of the screen to capture.
   * @param {boolean} [options.showCursor=true] - Show the cursor.
   * @param {boolean} [options.highlightClicks=false] - Highlight mouse clicks.
   * @param {number} [options.screenId=0] - Screen ID to capture.
   * @param {number} [options.audioDeviceId] - System audio device ID.
   * @param {string} [options.microphoneDeviceId] - Microphone device ID.
   * @param {string} [options.videoCodec="h264"] - Video codec to use.
   * @param {boolean} [options.enableHDR=false] - Enable HDR recording.
   * @param {boolean} [options.recordToFile=false] - Use the direct recording API.
   * @returns {Promise<void>} A promise that resolves when recording starts.
   * @throws {Error} If recording is already in progress or if the options are invalid.
   */
  async startRecording({
    fps = 30,
    cropArea = undefined,
    showCursor = true,
    highlightClicks = false,
    screenId = 0,
    audioDeviceId = undefined,
    microphoneDeviceId = undefined,
    videoCodec = "h264",
    enableHDR = false,
    recordToFile = false,
    audioOnly = false,
  }: Partial<RecordingOptions> = {}) {
    this.processId = getRandomId();
    // Stocke les options actuelles pour utilisation ultérieure
    this.currentOptions = {
      fps,
      cropArea,
      showCursor,
      highlightClicks,
      screenId,
      audioDeviceId,
      microphoneDeviceId,
      videoCodec,
      enableHDR,
      recordToFile,
      audioOnly,
    };
    
    return new Promise((resolve, reject) => {
      if (this.recorder !== undefined) {
        reject(new Error("Call `.stopRecording()` first"));
        return;
      }

      this.videoPath = temporaryFile({ extension: "mp4" });
      const recorderOptions: RecordingOptionsForScreenCaptureKit = {
        destination: fileUrl(this.videoPath as string),
        framesPerSecond: fps,
        showCursor,
        highlightClicks,
        screenId,
        audioDeviceId,
      };

      if (highlightClicks === true) {
        showCursor = true;
      }

      if (
        typeof cropArea === "object" &&
        (typeof cropArea.x !== "number" ||
          typeof cropArea.y !== "number" ||
          typeof cropArea.width !== "number" ||
          typeof cropArea.height !== "number")
      ) {
        reject(new Error("Invalid `cropArea` option object"));
        return;
      }

      if (videoCodec) {
        if (!videoCodecs.has(videoCodec)) {
          throw new Error(`Unsupported video codec specified: ${videoCodec}`);
        }

        recorderOptions.videoCodec = videoCodecs.get(videoCodec);
      }

      if (enableHDR) {
        if (!supportsHDR) {
          console.warn(
            "HDR requested but not supported on this macOS version. Falling back to SDR."
          );
        } else {
          recorderOptions.enableHDR = true;
        }
      }

      if (microphoneDeviceId) {
        if (!supportsMicrophoneCapture) {
          console.warn(
            "Microphone capture requested but requires macOS 15.0+. This feature will be ignored."
          );
        } else {
          recorderOptions.microphoneDeviceId = microphoneDeviceId;
        }
      }

      if (recordToFile) {
        if (!supportsDirectRecordingAPI) {
          console.warn(
            "Direct recording API requested but requires macOS 15.0+. Falling back to manual recording."
          );
        } else {
          recorderOptions.useDirectRecordingAPI = true;
        }
      }

      if (cropArea) {
        recorderOptions.cropRect = [
          [cropArea.x, cropArea.y],
          [cropArea.width, cropArea.height],
        ];
      }

      const timeout = setTimeout(resolve, 1000);
      this.recorder = execa(BIN, ["record", JSON.stringify(recorderOptions)]);

      this.recorder?.catch((error) => {
        clearTimeout(timeout);
        delete this.recorder;
        reject(error);
      });

      this.recorder?.stdout?.setEncoding("utf8");
      this.recorder?.stdout?.on("data", (data) => {
        console.log("From swift executable: ", data);
      });
    });
  }

  /**
   * Stops the ongoing recording and processes the video to merge audio tracks if needed.
   * @returns {Promise<string|null>} A promise that resolves with the path to the processed video file.
   * @throws {Error} If recording has not been started.
   */
  async stopRecording() {
    this.throwIfNotStarted();
    console.log("Arrêt de l'enregistrement");
    this.recorder?.kill();
    await this.recorder;
    console.log("Enregistrement arrêté");
    this.recorder = undefined;

    if (!this.videoPath) {
      return null;
    }

    let currentFile = this.videoPath;

    // Si nous avons plusieurs sources audio, nous devons les fusionner
    const hasMultipleAudioTracks = !!(
      this.currentOptions?.audioDeviceId && 
      this.currentOptions?.microphoneDeviceId
    );

    if (hasMultipleAudioTracks) {
      try {
        console.log("Fusion des pistes audio avec ffmpeg");
        this.processedVideoPath = temporaryFile({ extension: "mp4" });
        
        // Vérifier la structure du fichier avec ffprobe
        const { stdout: probeOutput } = await execa("ffprobe", [
          "-v", "error",
          "-show_entries", "stream=index,codec_type",
          "-of", "json",
          currentFile
        ]);
        
        const probeResult = JSON.parse(probeOutput);
        const streams = probeResult.streams || [];
        
        // Identifier les indices des flux audio et vidéo
        const audioStreams = streams
          .filter((stream: {codec_type: string; index: number}) => stream.codec_type === "audio")
          .map((stream: {index: number}) => stream.index);
          
        const videoStream = streams
          .find((stream: {codec_type: string; index: number}) => stream.codec_type === "video")?.index;
          
        if (audioStreams.length < 2 || videoStream === undefined) {
          console.log("Pas assez de pistes audio pour fusionner ou pas de piste vidéo");
        } else {
          const systemAudioIndex = audioStreams[0];
          const microphoneIndex = audioStreams[1];
          
          const filterComplex = `[0:${systemAudioIndex}]volume=1[a1];[0:${microphoneIndex}]volume=3[a2];[a1][a2]amerge=inputs=2[aout]`;
          
          // Traitement vidéo
          await execa("ffmpeg", [
            "-i", currentFile,
            "-filter_complex", filterComplex,
            "-map", "[aout]",
            "-map", `0:${videoStream}`,
            "-c:v", "copy",
            "-c:a", "aac",
            "-b:a", "256k",
            "-ac", "2",
            "-y",
            this.processedVideoPath
          ]);
          
          currentFile = this.processedVideoPath;
        }
      } catch (error) {
        console.error("Erreur lors de la fusion des pistes audio:", error);
      }
    }

    // Si audioOnly est activé, convertir en MP3
    if (this.currentOptions?.audioOnly) {
      try {
        console.log("Conversion en MP3");
        const audioPath = temporaryFile({ extension: "mp3" });
        
        await execa("ffmpeg", [
          "-i", currentFile,
          "-vn",
          "-c:a", "libmp3lame",
          "-b:a", "192k",
          "-y",
          audioPath
        ]);
        
        return audioPath;
      } catch (error) {
        console.error("Erreur lors de la conversion en MP3:", error);
        return currentFile;
      }
    }

    return currentFile;
  }
}

/**
 * Creates and returns a new instance of ScreenCaptureKit.
 * @returns {ScreenCaptureKit} A new instance of the screen recorder.
 */
export default function () {
  return new ScreenCaptureKit();
}

/**
 * Retrieves the video codecs available on the system.
 * @returns {Map<string, string>} A map of available video codecs.
 * @private
 */
function getCodecs() {
  const codecs = new Map([
    ["h264", "H264"],
    ["hevc", "HEVC"],
    ["proRes422", "Apple ProRes 422"],
    ["proRes4444", "Apple ProRes 4444"],
  ]);

  if (!supportsHevcHardwareEncoding) {
    codecs.delete("hevc");
  }

  return codecs;
}

/**
 * Retrieves the list of screens available for recording.
 * @returns {Promise<Array>} A promise that resolves with an array of objects representing the screens.
 * Each object contains the properties id, width, and height.
 */
export const screens = async () => {
  const { stderr } = await execa(BIN, ["list", "screens"]);

  try {
    return JSON.parse(stderr);
  } catch {
    return stderr;
  }
};

/**
 * Retrieves the list of system audio devices available for recording.
 * @returns {Promise<Array>} A promise that resolves with an array of objects representing the audio devices.
 * Each object contains the properties id, name, and manufacturer.
 */
export const audioDevices = async () => {
  const { stderr } = await execa(BIN, ["list", "audio-devices"]);

  try {
    return JSON.parse(stderr);
  } catch {
    return stderr;
  }
};

/**
 * Retrieves the list of microphone devices available for recording.
 * @returns {Promise<Array>} A promise that resolves with an array of objects representing the microphones.
 * Each object contains the properties id, name, and manufacturer.
 */
export const microphoneDevices = async () => {
  const { stderr } = await execa(BIN, ["list", "microphone-devices"]);

  try {
    return JSON.parse(stderr);
  } catch {
    return stderr;
  }
};

/**
 * Indicates whether the current system supports HDR capture.
 * @type {boolean}
 */
export const supportsHDRCapture = supportsHDR;

/**
 * Map of video codecs available on the system.
 * @type {Map<string, string>}
 */
export const videoCodecs = getCodecs();

# ======================
# File: src/types/file-url.d.ts
# ======================

declare module 'file-url' {
  /**
   * Convert a path to a file URL.
   * @param {string} filePath - Path to convert to file URL.
   * @returns {string} A properly formatted file URL.
   */
  function fileUrl(filePath: string): string;
  
  export default fileUrl;
} 