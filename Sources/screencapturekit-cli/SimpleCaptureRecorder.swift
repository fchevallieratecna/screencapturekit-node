import Foundation
import AVFoundation
import ScreenCaptureKit

/// Enregistreur simplifi√© pour tester la capture audio
class SimpleCaptureRecorder {
    private var stream: SCStream?
    private var assetWriter: AVAssetWriter?
    private var videoInput: AVAssetWriterInput?
    private var audioInput: AVAssetWriterInput?
    private var microphoneInput: AVAssetWriterInput?
    private var videoOutputDelegate: VideoOutputDelegate?
    private var audioOutputDelegate: AudioOutputDelegate?
    private var microphoneOutputDelegate: AudioOutputDelegate?
    private var isRecording = false
    private var outputURL: URL
    
    init(outputURL: URL) {
        self.outputURL = outputURL
        print("SimpleCaptureRecorder initialis√© avec URL: \(outputURL)")
    }
    
    func startRecording(screenId: CGDirectDisplayID, audioDeviceId: String?, microphoneDeviceId: String?) async throws {
        print("‚è±Ô∏è D√©marrage de l'enregistrement...")
        print("- √âcran: \(screenId)")
        print("- P√©riph√©rique audio: \(audioDeviceId ?? "aucun")")
        print("- Microphone: \(microphoneDeviceId ?? "aucun")")
        
        // V√©rifier les permissions d'enregistrement d'√©cran
        guard CGPreflightScreenCaptureAccess() else {
            throw NSError(domain: "ScreenCaptureError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Permission d'enregistrement d'√©cran refus√©e"])
        }
        
        // Cr√©er l'AssetWriter
        try setupAssetWriter(withAudio: audioDeviceId != nil, withMicrophone: microphoneDeviceId != nil)
        
        // Configurer le flux de capture
        try await setupCaptureStream(screenId: screenId, audioDeviceId: audioDeviceId, microphoneDeviceId: microphoneDeviceId)
        
        // D√©marrer la capture
        try await stream?.startCapture()
        isRecording = true
        print("‚úÖ Enregistrement d√©marr√© avec succ√®s")
    }
    
    func stopRecording() async throws {
        guard isRecording, let stream = self.stream else {
            print("‚ùå Aucun enregistrement en cours")
            return
        }
        
        print("‚è±Ô∏è Arr√™t de l'enregistrement...")
        
        // Arr√™ter la capture
        try await stream.stopCapture()
        print("‚úÖ Capture arr√™t√©e")
        
        // Finaliser les inputs
        videoInput?.markAsFinished()
        audioInput?.markAsFinished()
        microphoneInput?.markAsFinished()
        print("‚úÖ Inputs marqu√©s comme termin√©s")
        
        // Finaliser l'√©criture
        if let assetWriter = self.assetWriter {
            await assetWriter.finishWriting()
            print("‚úÖ √âcriture finalis√©e")
            
            // V√©rifier la taille du fichier
            let fileManager = FileManager.default
            if let attributes = try? fileManager.attributesOfItem(atPath: outputURL.path),
               let fileSize = attributes[.size] as? NSNumber {
                print("üìä Taille du fichier: \(fileSize.doubleValue / 1024.0) KB")
            } else {
                print("‚ö†Ô∏è Impossible de d√©terminer la taille du fichier")
            }
        }
        
        isRecording = false
        print("‚úÖ Enregistrement termin√©")
    }
    
    private func setupAssetWriter(withAudio: Bool, withMicrophone: Bool) throws {
        // V√©rifier si le fichier existe d√©j√† et le supprimer
        let fileManager = FileManager.default
        if fileManager.fileExists(atPath: outputURL.path) {
            try fileManager.removeItem(at: outputURL)
            print("üóëÔ∏è Ancien fichier supprim√©")
        }
        
        // Cr√©er l'asset writer pour MP4
        assetWriter = try AVAssetWriter(url: outputURL, fileType: .mp4)
        print("‚úÖ AssetWriter cr√©√© pour le fichier MP4")
        
        // Configurer le VideoInput
        let videoSettings: [String: Any] = [
            AVVideoCodecKey: AVVideoCodecType.h264,
            AVVideoWidthKey: 1920,
            AVVideoHeightKey: 1080,
            AVVideoCompressionPropertiesKey: [
                AVVideoAverageBitRateKey: 8_000_000,
                AVVideoProfileLevelKey: AVVideoProfileLevelH264HighAutoLevel
            ]
        ]
        
        videoInput = AVAssetWriterInput(mediaType: .video, outputSettings: videoSettings)
        videoInput?.expectsMediaDataInRealTime = true
        
        // Configurer l'AudioInput avec des param√®tres de haute qualit√© (audio syst√®me)
        if withAudio {
            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 48000,
                AVNumberOfChannelsKey: 2,
                AVEncoderBitRateKey: 256000
            ]
            
            audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioInput?.expectsMediaDataInRealTime = true
        }
        
        // Configurer l'entr√©e microphone
        if withMicrophone {
            let micSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVSampleRateKey: 48000,
                AVNumberOfChannelsKey: 1,
                AVEncoderBitRateKey: 128000
            ]
            
            microphoneInput = AVAssetWriterInput(mediaType: .audio, outputSettings: micSettings)
            microphoneInput?.expectsMediaDataInRealTime = true
        }
        
        // Ajouter les inputs √† l'asset writer
        if let videoInput = videoInput, assetWriter?.canAdd(videoInput) == true {
            assetWriter?.add(videoInput)
            print("‚úÖ VideoInput ajout√©")
        }
        
        if let audioInput = audioInput, assetWriter?.canAdd(audioInput) == true {
            assetWriter?.add(audioInput)
            print("‚úÖ AudioInput (syst√®me) ajout√©")
        }
        
        if let microphoneInput = microphoneInput, assetWriter?.canAdd(microphoneInput) == true {
            assetWriter?.add(microphoneInput)
            print("‚úÖ MicrophoneInput ajout√©")
        }
        
        // D√©marrer l'√©criture
        assetWriter?.startWriting()
        assetWriter?.startSession(atSourceTime: .zero)
        print("‚úÖ Session d'√©criture d√©marr√©e")
    }
    
    private func setupCaptureStream(screenId: CGDirectDisplayID, audioDeviceId: String?, microphoneDeviceId: String?) async throws {
        // R√©cup√©rer le contenu partageable
        let content = try await SCShareableContent.current
        
        // Trouver l'√©cran demand√©
        guard let display = content.displays.first(where: { $0.displayID == screenId }) else {
            throw NSError(domain: "ScreenCaptureError", code: 2, userInfo: [NSLocalizedDescriptionKey: "√âcran non trouv√©: \(screenId)"])
        }
        
        // Cr√©er le filtre
        let filter = SCContentFilter(display: display, excludingApplications: [], exceptingWindows: [])
        print("‚úÖ Filtre de contenu cr√©√©")
        
        // Configurer le stream
        let configuration = SCStreamConfiguration()
        configuration.width = 1920
        configuration.height = 1080
        configuration.minimumFrameInterval = CMTime(value: 1, timescale: 60) // 60 FPS
        configuration.showsCursor = true
        
        // Configurer l'audio syst√®me
        if audioDeviceId != nil {
            configuration.capturesAudio = true
            configuration.excludesCurrentProcessAudio = false
            print("‚úÖ Capture audio syst√®me activ√©e")
        }
        
        // Configurer le microphone (si disponible et si nous sommes sur macOS 15.0+)
        if microphoneDeviceId != nil {
            if #available(macOS 15.0, *) {
                // Sur macOS Sequoia, utiliser l'API native pour le microphone
                configuration.captureMicrophone = true
                print("‚úÖ Capture microphone activ√©e avec l'API native de macOS 15")
            } else {
                // La propri√©t√© n'existe pas dans les anciennes versions, utilisons une approche alternative
                print("‚ö†Ô∏è Support du microphone non impl√©ment√© - n√©cessite macOS 15.0+")
                // Nous allons quand m√™me essayer de capturer le micro via l'audio syst√®me
                configuration.capturesAudio = true
                configuration.excludesCurrentProcessAudio = false
            }
        }
        
        // Cr√©er les d√©l√©gu√©s
        videoOutputDelegate = VideoOutputDelegate(videoInput: videoInput)
        
        if audioDeviceId != nil {
            audioOutputDelegate = AudioOutputDelegate(audioInput: audioInput, isMicrophone: false)
        }
        
        if microphoneDeviceId != nil {
            if #available(macOS 15.0, *) {
                microphoneOutputDelegate = AudioOutputDelegate(audioInput: microphoneInput, isMicrophone: true)
            } else {
                print("‚ö†Ô∏è Le support du microphone n√©cessite macOS 15.0+, le microphone sera ignor√©")
            }
        }
        
        // Connecter les d√©l√©gu√©s entre eux pour la synchronisation
        if let videoDelegate = videoOutputDelegate {
            if let audioDelegate = audioOutputDelegate {
                videoDelegate.setAudioDelegate(audioDelegate)
            }
            
            if #available(macOS 15.0, *) {
                if let microphoneDelegate = microphoneOutputDelegate {
                    videoDelegate.setMicrophoneDelegate(microphoneDelegate)
                }
            }
            
            print("‚úÖ D√©l√©gu√©s connect√©s pour la synchronisation")
        }
        
        // Cr√©er le stream
        stream = SCStream(filter: filter, configuration: configuration, delegate: nil)
        
        // Ajouter les sorties
        if let stream = stream {
            try stream.addStreamOutput(videoOutputDelegate!, type: .screen, sampleHandlerQueue: DispatchQueue(label: "video-queue"))
            print("‚úÖ Sortie vid√©o ajout√©e au stream")
            
            if audioDeviceId != nil, let audioDelegate = audioOutputDelegate {
                try stream.addStreamOutput(audioDelegate, type: .audio, sampleHandlerQueue: DispatchQueue(label: "audio-queue"))
                print("‚úÖ Sortie audio syst√®me ajout√©e au stream")
            }
            
            if #available(macOS 15.0, *) {
                if microphoneDeviceId != nil, let microphoneDelegate = microphoneOutputDelegate {
                    // Sur macOS 15+, utiliser le type .microphone correct
                    try stream.addStreamOutput(microphoneDelegate, type: .microphone, sampleHandlerQueue: DispatchQueue(label: "microphone-queue"))
                    print("‚úÖ Sortie microphone ajout√©e au stream avec type .microphone (macOS 15)")
                }
            }
        }
    }
}

// D√©l√©gu√© pour la sortie vid√©o
class VideoOutputDelegate: NSObject, SCStreamOutput {
    private let videoInput: AVAssetWriterInput?
    private var firstSampleTime = CMTime.zero
    private var audioDelegate: AudioOutputDelegate?
    private var microphoneDelegate: AudioOutputDelegate?
    
    init(videoInput: AVAssetWriterInput?) {
        self.videoInput = videoInput
    }
    
    func setAudioDelegate(_ delegate: AudioOutputDelegate) {
        self.audioDelegate = delegate
    }
    
    @available(macOS 15.0, *)
    func setMicrophoneDelegate(_ delegate: AudioOutputDelegate) {
        self.microphoneDelegate = delegate
    }
    
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        guard type == .screen, let videoInput = videoInput, videoInput.isReadyForMoreMediaData, sampleBuffer.isValid else {
            return
        }
        
        // Pour la premi√®re frame, enregistrer le timestamp
        if firstSampleTime == .zero {
            firstSampleTime = sampleBuffer.presentationTimeStamp
            print("‚è±Ô∏è Premier √©chantillon vid√©o re√ßu √† \(firstSampleTime.seconds)s")
            
            // Informer les d√©l√©gu√©s audio du premier timestamp vid√©o
            audioDelegate?.setFirstVideoSampleTime(firstSampleTime)
            
            if #available(macOS 15.0, *) {
                microphoneDelegate?.setFirstVideoSampleTime(firstSampleTime)
            }
        }
        
        // Ajuster le timestamp
        let relativeTime = sampleBuffer.presentationTimeStamp - firstSampleTime
        
        // S'assurer que le frame est complet
        guard let attachments = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, createIfNecessary: false) as? [[SCStreamFrameInfo: Any]],
              let status = attachments.first?[SCStreamFrameInfo.status] as? Int,
              SCFrameStatus(rawValue: status) == .complete else {
            return
        }
        
        // Retemporiser le buffer
        let timing = CMSampleTimingInfo(
            duration: sampleBuffer.duration,
            presentationTimeStamp: relativeTime,
            decodeTimeStamp: .invalid
        )
        
        do {
            let retimedBuffer = try CMSampleBuffer(copying: sampleBuffer, withNewTiming: [timing])
            videoInput.append(retimedBuffer)
        } catch {
            print("‚ö†Ô∏è Erreur lors de la retemporisation vid√©o: \(error)")
        }
    }
}

// D√©l√©gu√© pour la sortie audio (syst√®me ou microphone)
class AudioOutputDelegate: NSObject, SCStreamOutput {
    private let audioInput: AVAssetWriterInput?
    private var firstSampleTime = CMTime.zero
    private var firstVideoSampleTime = CMTime.zero
    private var earlyAudioSamples = [CMSampleBuffer]()
    private var hasReceivedVideoSample = false
    private var isMicrophone: Bool
    
    init(audioInput: AVAssetWriterInput?, isMicrophone: Bool = false) {
        self.audioInput = audioInput
        self.isMicrophone = isMicrophone
    }
    
    func setFirstVideoSampleTime(_ time: CMTime) {
        firstVideoSampleTime = time
        hasReceivedVideoSample = true
        
        // Traiter les √©chantillons audio pr√©coces
        processEarlyAudioSamples()
    }
    
    private func processEarlyAudioSamples() {
        for sample in earlyAudioSamples {
            processSampleBuffer(sample)
        }
        let count = earlyAudioSamples.count
        earlyAudioSamples.removeAll()
        print("‚úÖ \(count) √©chantillons audio pr√©coces trait√©s" + (isMicrophone ? " (microphone)" : " (syst√®me)"))
    }
    
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        // V√©rifier le type appropri√© (audio ou microphone)
        let expectedType: SCStreamOutputType
        
        if isMicrophone {
            if #available(macOS 15.0, *) {
                expectedType = .microphone
            } else {
                expectedType = .audio // Fallback pour les anciennes versions de macOS
            }
        } else {
            expectedType = .audio
        }
        
        guard type == expectedType, let audioInput = audioInput, audioInput.isReadyForMoreMediaData, sampleBuffer.isValid else {
            return
        }
        
        // Si c'est le premier √©chantillon audio, enregistrer le timestamp
        if firstSampleTime == .zero {
            firstSampleTime = sampleBuffer.presentationTimeStamp
            print("‚è±Ô∏è Premier √©chantillon \(isMicrophone ? "microphone" : "audio syst√®me") re√ßu √† \(firstSampleTime.seconds)s")
        }
        
        // Si la vid√©o n'a pas encore commenc√©, stocker les √©chantillons
        if !hasReceivedVideoSample {
            earlyAudioSamples.append(sampleBuffer)
            return
        }
        
        processSampleBuffer(sampleBuffer)
    }
    
    private func processSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        guard let audioInput = audioInput, audioInput.isReadyForMoreMediaData else {
            return
        }
        
        // Ajuster le timestamp par rapport au premier √©chantillon vid√©o
        let relativeTime = sampleBuffer.presentationTimeStamp - firstVideoSampleTime
        
        // Retemporiser le buffer
        let timing = CMSampleTimingInfo(
            duration: sampleBuffer.duration,
            presentationTimeStamp: relativeTime,
            decodeTimeStamp: .invalid
        )
        
        do {
            let retimedBuffer = try CMSampleBuffer(copying: sampleBuffer, withNewTiming: [timing])
            audioInput.append(retimedBuffer)
            
            let source = isMicrophone ? "microphone" : "audio syst√®me"
            print("‚úÖ √âchantillon \(source) ajout√© √† \(relativeTime.seconds)s")
        } catch {
            print("‚ö†Ô∏è Erreur lors de la retemporisation audio \(isMicrophone ? "(microphone)" : "(syst√®me)"): \(error)")
        }
    }
} 